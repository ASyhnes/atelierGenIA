{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e390e0ed-fd98-4ecb-b50b-aa43a7cc9b9f",
   "metadata": {},
   "source": [
    "### Cours IA GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92c10ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "529eb27d-246d-4351-b5a3-3cb7dba3c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d51b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10)\n",
      "Requirement already satisfied: urllib3<3 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2024.8.30)\n",
      "Requirement already satisfied: requests in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: anyio<5 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.7.0)\n",
      "Requirement already satisfied: distro<2 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: h11<0.15 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: annotated-types in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: typing-extensions<5 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.10.3)\n",
      "Requirement already satisfied: jiter<1 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: openai in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.57.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: httpx<0.28 in c:\\users\\syhnes\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.27.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --upgrade-strategy eager \"regex\" \"charset-normalizer<4\" \"idna\" \"urllib3<3\" \"certifi\" \"requests\" \"anyio<5\" \"distro<2\" \"sniffio\" \"h11<0.15\" \"httpcore==1.*\" \"annotated-types\" \"typing-extensions<5\" \"pydantic-core==2.27.1\" \"pydantic<3\" \"jiter<1\" \"tqdm\" \"colorama\" \"openai\" \"tiktoken\" \"httpx<0.28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd0a3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a391d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature=0.7, max_tokens=100, top_p=1.0):\n",
    "    # Appeler l'API pour créer une complétion de chat\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],  # Message utilisateur\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    # Retourner le texte généré\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46eedc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse générée :\n",
      " Les chats ronronnent pour exprimer leur contentement, leur bien-être ou leur besoin de réconfort.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Expliquez en une phrase pourquoi les chats ronronnent.\"\n",
    "response = generate_text(prompt)\n",
    "print(\"Réponse générée :\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "268c3731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résumé généré :\n",
      " L'IA consiste en la création de systèmes informatiques capables d'effectuer des tâches nécessitant une intelligence humaine.\n"
     ]
    }
   ],
   "source": [
    "passage = \"\"\"\n",
    "L’intelligence artificielle (IA) est un domaine de l’informatique qui se concentre sur la création de systèmes capables de réaliser des tâches nécessitant habituellement une intelligence humaine, telles que la vision par ordinateur et la prise de décision.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Résumé ce texte en 20 mots ou moins :\\n{passage}\"\n",
    "summary = generate_text(prompt, temperature=0.7, max_tokens=50)\n",
    "print(\"Résumé généré :\\n\", summary)\n",
    "\n",
    "# Modifier les paramètres pour observer les différences :\n",
    "low_temp = generate_text(prompt, temperature=0.2, max_tokens=50)\n",
    "high_temp = generate_text(prompt, temperature=2, max_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af80371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résumé avec température basse (0.2) :\n",
      " L'IA est un domaine informatique créant des systèmes capables de réaliser des tâches nécessitant une intelligence humaine.\n",
      "\n",
      "Résumé avec température élevée (0.9) :\n",
      " L'IA consiste à créer des systèmes informatiques pour effectuer des tâches nécessitant une intelligence humaine, comme la vision et la décision.\n"
     ]
    }
   ],
   "source": [
    "low_temp = generate_text(prompt, temperature=0.2, max_tokens=50)\n",
    "high_temp = generate_text(prompt, temperature=0.8, max_tokens=50)\n",
    "\n",
    "print(\"Résumé avec température basse (0.2) :\\n\", low_temp)\n",
    "print(\"\\nRésumé avec température élevée (0.9) :\\n\", high_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea70f11",
   "metadata": {},
   "source": [
    "### Ce que fait le code :\n",
    "Différence entre températures :\n",
    "\n",
    "Génère deux résumés avec des températures différentes :\n",
    "0.2 pour un résumé conservateur et précis.\n",
    "0.8 pour un résumé plus créatif et varié.\n",
    "### Exploration créative :\n",
    "\n",
    "Crée une courte histoire basée sur un prompt incluant des contraintes spécifiques (chat, horloge cassée, tempête).\n",
    "Utilise une température élevée (0.9) pour encourager la créativité.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1965c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histoire générée :\n",
      " Il était une fois un petit chat nommé Whiskers qui vivait dans une vieille maison près de la forêt. Un jour, une violente tempête éclata, faisant trembler les murs de la maison et faisant voler en éclats une horloge ancienne accrochée au mur.\n",
      "\n",
      "Whiskers, curieux et agile, se précipita pour inspecter les dégâts. Il trouva l'horloge cassée par terre, ses aiguilles tordues et son mécanisme détraqué. Intrigué par cette découverte, le chat entreprit de jouer avec les morceaux de l'horloge, les faisant tourner et tourbillonner autour de lui.\n",
      "\n",
      "Soudain, quelque chose de magique se produisit. Au moment où Whiskers toucha les aiguilles tordues, le temps sembla se figer. La tempête semblait suspendue dans les airs, les gouttes de pluie en suspension, figées comme des cristaux de glace.\n",
      "\n",
      "Whiskers, émerveillé par ce phénomène, continua de jouer avec l'horloge cassée, essayant différentes combinaisons avec les aiguilles. À chaque mouvement, le temps semblait se plier à sa volonté, s'accélérant ou ralentissant selon son bon plaisir.\n",
      "\n",
      "Finalement, après de nombreuses expérimentations, Whiskers fut capable de réparer l'horloge et de remettre les aiguilles en place. À cet instant, le temps reprit son cours normal et la tempête reprit de plus belle, secouant la maison de ses rafales de vent et de pluie.\n",
      "\n",
      "Whiskers, satisfait de sa découverte et de son aventure hors du commun, se blottit confortablement dans un coin de la maison, observant le spectacle tumultueux à travers la fenêtre, reconnaissant pour sa rencontre avec l'horloge magique et la leçon sur le temps qu'elle lui avait enseignée.\n"
     ]
    }
   ],
   "source": [
    "creative_prompt = \"Raconte une histoire impliquant un chat, une horloge cassée et une tempête.\"\n",
    "story = generate_text(creative_prompt, temperature=0.9, max_tokens=1000)\n",
    "print(\"Histoire générée :\\n\", story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f5520",
   "metadata": {},
   "source": [
    "Analyse :\n",
    "Exploration créative :\n",
    "\n",
    "Prompt : fournir une consigne claire et spécifique pour générer une histoire.\n",
    "Température : Une valeur élevée (0.9) favorise la créativité et introduit des éléments imprévisibles.\n",
    "Max Tokens :\n",
    "\n",
    "Limité à 100 tokens pour éviter une sortie trop longue tout en laissant suffisamment d'espace pour une histoire courte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5efeeae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### GPT-3.5-Turbo\t\n",
    "Modèle utilisé par ChatGPT, performant pour de nombreux cas d'utilisation non liés au chat. Disponible via l'API publique. \n",
    "\n",
    "### GPT-4\t\n",
    "Amélioration de GPT-3.5, capable de comprendre et générer du langage naturel et du code. Disponible en version bêta limitée. \n",
    "\n",
    "### GPT-4o\t\n",
    "Modèle multimodal capable de traiter et générer du texte, des images et du son. Plus rapide et moins coûteux que GPT-4. Disponible pour tous les utilisateurs avec certaines limitations. \n",
    "\n",
    "### GPT-4o \n",
    "Mini\tVersion allégée de GPT-4o, remplaçant GPT-3.5 Turbo dans l'interface ChatGPT. Plus abordable et efficace, idéale pour les entreprises et développeurs avec des ressources limitées. \n",
    "\n",
    "### o1\t\n",
    "Modèle conçu pour résoudre des problèmes complexes en prenant plus de temps pour formuler ses réponses, améliorant ainsi la précision. Excelle en programmation compétitive, mathématiques et raisonnement scientifique. Disponible pour les abonnés ChatGPT Pro. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030931c5",
   "metadata": {},
   "source": [
    "## Suite avec d'autres modèles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68085841",
   "metadata": {},
   "source": [
    "###                                       GPT4-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8737659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse générée :\n",
      " Les chats ronronnent généralement pour exprimer le contentement, mais aussi pour se calmer eux-mêmes lorsqu'ils sont stressés ou malades.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_text2(prompt, temperature=0.7, max_tokens=100, top_p=1.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "prompt = \"Expliquez en une phrase pourquoi les chats ronronnent.\"\n",
    "response = generate_text2(prompt)\n",
    "print(\"Réponse générée :\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c942552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histoire générée :\n",
      " Il était une fois, dans un petit village niché au creux des montagnes, une vieille maison qui semblait avoir été oubliée par le temps. Les murs étaient recouverts de lierre, et le toit s'inclinait légèrement, comme sous le poids des années. Dans cette maison vivait un chat du nom de Mistral, connu pour son pelage couleur de nuage et ses yeux d'un vert envoûtant.\n",
      "\n",
      "Mistral n'était pas un chat comme les autres. Il avait une fascination particulière pour les objets anciens, et son préféré était une vieille horloge en bois sculpté qui trônait fièrement dans le salon. L'horloge avait cessé de fonctionner bien des années auparavant, mais Mistral aimait se blottir contre elle, comme s'il espérait, par sa simple présence, la ramener à la vie.\n",
      "\n",
      "Un soir, alors que le vent commençait à souffler avec force, annonçant l'arrivée d'une tempête, Mistral s'allongea près de l'horloge, ses paupières mi-closes. À l'extérieur, les nuages sombres s'amoncelaient, cachant peu à peu les étoiles. Soudain, un éclair zébra le ciel, suivi d'un grondement de tonnerre qui fit vibrer les murs de la maison.\n",
      "\n",
      "La tempête fut d'une rare violence, les vents hurlant à travers les fissures de la maison, la pluie tambourinant sur le toit. Pourtant, Mistral resta près de l'horloge, comme pour la protéger de l’agitation extérieure. Tout à coup, dans un éclat de lumière, une goutte d'eau s'insinua à travers le plafond et tomba directement sur le mécanisme de l'horloge.\n",
      "\n",
      "Alors que le tonnerre s'éloignait, Mistral sentit une vibration sous sa patte. Il recula d'un bond, les yeux écarquillés, en observant l'inimaginable : l'horloge se remit lentement à faire tic-tac. La grande aiguille se mit en mouvement, avançant une seconde à la fois, comme si la tempête avait insufflé une nouvelle vie à l'objet silencieux.\n",
      "\n",
      "Envoûté, Mistral suivit des yeux la danse des aiguilles, émerveillé de voir son vieil ami revenir à la vie. Il comprit alors que la tempête n'avait pas seulement apporté le chaos, mais aussi la transformation. Ce qui était brisé pouvait parfois être réparé par l'impensable force de la nature.\n",
      "\n",
      "La tempête passa, laissant derrière elle un ciel pur et scintillant. L'horloge, désormais en marche, semblait sourire d'un tic-tac régulier, et Mistral, le cœur apaisé, se blottit contre elle une fois de plus, satisfait d'avoir été le témoin d'un petit miracle nocturne.\n"
     ]
    }
   ],
   "source": [
    "creative_prompt = \"Raconte une histoire impliquant un chat, une horloge cassée et une tempête.\"\n",
    "story = generate_text2(creative_prompt, temperature=0.9, max_tokens=1000)\n",
    "print(\"Histoire générée :\\n\", story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e6238",
   "metadata": {},
   "source": [
    "### O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d8f92c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpliquez en une phrase pourquoi les chats ronronnent.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRéponse générée :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[87], line 2\u001b[0m, in \u001b[0;36mgenerate_text3\u001b[1;34m(prompt, temperature, max_tokens, top_p)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text3\u001b[39m(prompt, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mo1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Utiliser le modèle o1\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\Syhnes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Syhnes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Syhnes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1279\u001b[0m     )\n\u001b[1;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Syhnes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Syhnes\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1070\u001b[0m )\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `o1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "def generate_text3(prompt, temperature=0.7, max_tokens=100, top_p=1.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"o1\",  # Utiliser le modèle o1\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "prompt = \"Expliquez en une phrase pourquoi les chats ronronnent.\"\n",
    "response = generate_text3(prompt)\n",
    "print(\"Réponse générée :\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
